<!DOCTYPE html><html lang="en" ><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Jekyll v4.2.1" /><meta property="og:title" content="[밑시딥] 3. 신경망" /><meta name="author" content="Junil" /><meta property="og:locale" content="en_US" /><meta name="description" content="밑바닥부터 시작하는 딥러닝을 보고 공부한 내용을 정리합니다." /><meta property="og:description" content="밑바닥부터 시작하는 딥러닝을 보고 공부한 내용을 정리합니다." /><link rel="canonical" href="http://localhost:4000/2021/12/28/dls-3-neural-network" /><meta property="og:url" content="http://localhost:4000/2021/12/28/dls-3-neural-network" /><meta property="og:site_name" content="Sidey" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-12-28T16:27:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[밑시딥] 3. 신경망" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@Junil" /> <script type="application/ld+json"> {"author":{"@type":"Person","name":"Junil"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2021/12/28/dls-3-neural-network"},"@type":"BlogPosting","description":"밑바닥부터 시작하는 딥러닝을 보고 공부한 내용을 정리합니다.","url":"http://localhost:4000/2021/12/28/dls-3-neural-network","headline":"[밑시딥] 3. 신경망","dateModified":"2021-12-28T16:27:00+09:00","datePublished":"2021-12-28T16:27:00+09:00","@context":"https://schema.org"}</script><title> [밑시딥] 3. 신경망 - Sidey</title><link rel="shortcut icon" href="/favicon.png"><link rel="alternate" type="application/atom+xml" title="Sidey" href="/atom.xml"><link rel="alternate" type="application/json" title="Sidey" href="http://localhost:4000/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" /><style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}</style></head><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><body><main role="main"><header role="banner"> <!--<h1 class="logo">Sidey</h1>--><nav role="navigation"><ul><li><a href="/" >Writing</a></li><li><a href="/about" >About</a></li><li><a href="/search" >Search</a></li><li><a href="/atom.xml" >Rss</a></li></ul></nav></header><section class="post"><h2>[밑시딥] 3. 신경망</h2><blockquote><p>밑바닥부터 시작하는 딥러닝을 보고 공부한 내용을 정리합니다.</p></blockquote><h2 id="퍼셉트론">퍼셉트론</h2><p>CHAPTER 2에서 공부한 다중 퍼셉트론은 XOR같은 복잡한 함수도 표현할 수 있었다. 그렇지만 weight를 사람이 직접 설정해야 한다는 문제가 있다. ANS, OR게이트 정도의 문제는 진리표를 보면서 인간이 적절한 weight를 입력하는게 가능했지만 더 복잡한 문제는 그런 방법으로 해결하기 어렵다. 신경망은 weight의 적절한 값을 데이터로부터 학습한다. CHAPTER 3에서는 신경망이 입력 데이터가 무엇인지 식별하는 처리과정을 알아본다.</p><h2 id="퍼셉트론-복습">퍼셉트론 복습</h2><p><img src="/assets/images/how-to-perform-classification-using-a-neural-network-a-simple-perceptron-example_rk_aac_image1.webp" alt="" /> 위와 같은 구조의 네트워크는 아래의 식으로 나타낼 수 있다.<br /> \(y= \begin{cases} 0(w_1x_1+w_2x_2+b\le0) \\ 1(w_1x_1+w_2x_2+b&gt;0) \end{cases}\)<br /> 위 식에서 조건 분기의 동작을 함수로 나타내면 <br /> \(y = h(w_1x_1+w_2x_2+b)\)<br /> 함수 h를 다시 표현하면 아래와같이 표현할 수 있다.<br /> \(h(x) = \begin{cases} 0(x\le0) \\ 1(x&gt;0) \end{cases}\)</p><h2 id="활성화함수">활성화함수</h2><p>위에서 본 $h(x)$ 처럼 입력신호의 총합을 출력신호로 변환하는 함수를 활성화함수(activation funcion)이라고 부른다. 이름 그대로 입력신호의 총합이 활성화를 일으키는지 정하는 역할을 한다.</p><h2 id="계단함수">계단함수</h2><p>앞서 설명된 임계치보다 작으면 0, 크면 1을 반환 \(h(x) = \begin{cases} 0(x\le0) \\ 1(x&gt;0) \end{cases}\) <img src="/assets/images/image-1.png" alt="" /></p><h2 id="sigmoid-함수">Sigmoid 함수</h2><p>자연상수를 이용하여 입력신호를 0~1 사이의 실수값으로 반환 \(h(x) = {1 \over 1+e^{-x}}\) <img src="/assets/images/image-2.png" alt="" /></p><h2 id="relu">ReLU</h2><p>나중에 나오는 Gradient Venising 문제를 해결하기 위해 제안된 활성화함수 \(h(x) = \max(0, x)\) <img src="/assets/images/image-3.png" alt="" /></p><h2 id="code">code</h2><script src="https://gist.github.com/kimjunil/3b83c6e7ad66543715f38c04fbc5bdf5.js"></script><h2 id="다차원-배열의-계산과-신경망">다차원 배열의 계산과 신경망</h2><p>신경망은 행렬의 곱으로 계산을 수행할 수 있다. 이때 X와 W의 대응하는 차원의 원소 수가 같아야한다 <img src="/assets/images/image-4.png" alt="" /></p><pre><code class="language-python">X = np.array([1,2]) # shape -&gt; (2,)
W = np.array([[1,3,5], [2,4,6]]) # shape -&gt; (2,3)
Y = np.dot(X, W)
</code></pre><h2 id="3층-신경망-구현하기">3층 신경망 구현하기</h2><p><img src="/assets/images/image-5.png" alt="" /> \(a^{(1)}_1 = w^{(1)}_{11}x_1 + w^{(1)}_{12}x_2 + b^{(1)}_1 \\ a^{(1)}_2 = w^{(1)}_{21}x_1 + w^{(1)}_{22}x_2 + b^{(1)}_2 \\ a^{(1)}_3 = w^{(1)}_{31}x_1 + w^{(1)}_{32}x_2 + b^{(1)}_3 \\\) 행렬로 표현하면 아래와 같이 표현할 수 있다 \(A = XW^{(1)}+B^{(1)}\)</p><pre><code class="language-python">X = np.array([1.0, 2.0])
W1 = np.array([0.1, 0.3, 0.5], [0.2, 0.4, 0.6])
B1 = np.array([0.1, 0.2, 0.3])

A1 = np.dot(X, W1)+B1
</code></pre><h2 id="code-1">code</h2><script src="https://gist.github.com/kimjunil/229699e592ca2ce152ef1b3bcdbba15c.js"></script><h2 id="출력층-설계하기">출력층 설계하기</h2><p>분류/회귀 중 어떤 문제냐에 따라 출력층에서 사용하는 활성화함수가 달라진다. 일반적으로 회귀에는 항등함수, 분류에는 소프트맥스 함수를 사용한다. 항등함수는 입력신호 그대로 출력을 하고 소프트맥스 함수는 아래와 같은 식을 사용한다. \(y_k = {\exp(a_k)\over\sum\limits_{i=1}^n\exp(a_i)}\)</p><h2 id="소프트맥스-함수-구현">소프트맥스 함수 구현</h2><p>소프트맥스 함수는 지수함수를 사용하기 때문에 식대로 구현하면 오버플로우가 발생한다. 이 문제를 해결하기위해 수식을 개선한다 \(y_k = {\exp(a_k)\over\sum\limits_{i=1}^n\exp(a_i)}={C\exp(a_k)\over C\sum\limits_{i=1}^n\exp(a_i)} \\ = {\exp(a_k+\log{C})\over\sum\limits_{i=1}^n\exp(a_i+\log{C})}\\ = {\exp(a_k+C')\over\sum\limits_{i=1}^n\exp(a_i+C')}\) 위 식은 지수함수를 계산할 때 어떤 정수를 더하거나 빼도 결과는 바뀌지 않는다는 것을 보여준다. $C’$에는 어떤 값을 대입해도 되지만 오버플로우를 막을 목적으로는 입력신호 중 최댓값을 이용하는 것이 일반적이다.</p><pre><code class="language-python">def softmax(a):
    c = np.max(a)
    exp_a = mp.exp(a-c)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    
    return y
</code></pre><span class="meta"><time datetime="2021-12-28T16:27:00+09:00">December 28, 2021</time> &middot; <a href="/tag/Machine Learning">Machine Learning</a>, <a href="/tag/Deep Learning">Deep Learning</a></span></section></main></body></html>
